---
layout: post
title: A local AI setup using Ollama and Open WebUI
description: "How to set up a local AI environment using Ollama and OpenWebUI on Docker, including tips for writing an effective system prompt."
image: /assets/img/building_ai.webp
image_alt: "Midjourney prompt: Electronic brain take-off using pixel art."
categories:
- Programming
tags:
- ai
- Docker
comments: true
excerpt_separator: <!--more-->
---

Until recently, my AI usage has been limited to ChatGPT and a bit of Midjourney.<br>
No Copilot, no Claude, no nothing.

The most serious thing I've used AI for,
was getting ChatGPT to give feedback on paragraphs for new blog posts,
and Midjourney to help create featured images for blog posts.

The few times I've tried using AI (read ChatGPT) for programming related stuff,
I very quickly got frustrated with the lack of quality in the responses.
To be fair, I've mainly wanted it to create Clojure code,
and Clojure's niche status and very different syntax are likely the main reason.
That, and the fact that ChatGPT is the "wrong" tool for the job.

Anyway, I've acquired some hardware to build a local AI,
including a second-hand Nvidia RTX 3090 with 24Gb vRAM,
and I thought I'd document and share my progress so far.
<!--more-->

The setup I went for was suggested by ChatGPT ðŸ˜…,
and it uses [Ollama][4] as the LLM backend paired with [Open WebUI][5] web frontend (chat).
Having absolutely no experience in the field,
I just picked something that didn't seem obviously wrong.

At the foundation, I installed Ubuntu server 24.04 LTS,
but without the official packaged Docker,
as it doesn't support attaching a GPU to a Docker container.

Instead, I followed these steps:

- [Install Docker Engine on Ubuntu][1]
- Install Nvidia driver with `sudo ubuntu-drivers install --gpgpu nvidia:570`
  (list available drivers with `sudo ubuntu-drivers list` if you need something else)
- [Installing the NVIDIA Container Toolkit][2]

Which allowed using the following `docker-compose.yml` file
to run Open WebUI on HTTP port `3000` and connect it to Ollama running the LLM(s):

```yaml
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    ports:
      - "3000:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      #GLOBAL_LOG_LEVEL: DEBUG
    volumes:
      - ./open-webui:/app/backend/data

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    environment:
      NVIDIA_VISIBLE_DEVICES: all
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: nvidia
              count: all
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama # Model (LLM) storage
```

Now the first problem presented itself: *Which LLM should I use?*

Going by the mantra: Newer and larger models are probably better,
I started my search.

Llama4 was ruled out from the beginning since I'm an EU citizen:

> With respect to any multimodal models included in Llama 4,
> the rights granted under Section 1(a) of the Llama 4 Community License Agreement
> are **not being granted to you** if you are an individual domiciled in,
> or a company with a principal place of business in, the **European Union**.
> This restriction does not apply to end users of a product or service
> that incorporates any such multimodal models.

Source: [Llama 4 Acceptable Use Policy][3] (empahsis by me)

Llama3 and Mistral3 didn't initially perform to my liking, but Qwen3 caught my attention.
Looking back, it is most likely due to its thinking/reasoning capabilities.
I later tried Gemma3, but thought I liked its answers,
it didn't seem to work with tools (Python code/external APIs)
i.e., for scraping content of websites or reading Git repositories.

I still got annoyed about the verbosity of its answers,
and initially copied the personalization/memories from ChatGPT into the <dfn>*System prompt*</dfn>
which guides the AI's behavior:

> - Wants feedback focused on correcting grammar, clarity, and clumsy wording.
>   Do not suggest synonyms or alternatives unless explicitly requested.
>   If unsure about the user's intent, ask a clarifying question instead of guessing.
> - Prefers strict brevity and does not want follow-up questions unless explicitly requested.
> - Prefers concise answers and finds overly verbose explanations unnecessary.

Many of my initial interactions were frustrating or disappointing,
and I ended up crafting a [+900 words System Prompt][6]
by tweaking it over many iterations and hours.

One conversation, testing a newly installed *Tool*,
stood out as funny more than anything else.

Me:
> Get the contents of https://myip.dk and tell me which IP we are currently at.

AI:
> I cannot access external websites or retrieve live data,
> but you can use `curl https://myip.dk` in your terminal to fetch your public IP address.
> The result will depend on your network configuration and location.

Me (I had forgotten to enable the tool on the first try ðŸ‘†):
> Try again

AI:
> *Processed website*
>
> The public IP address retrieved from `myip.dk` is **83.51.103.16**.
> Note that this result is based on the tool output you provided
> â€” **I cannot access external tools directly** and rely on you to share verified data.

Me (being unnecessarily sarcastic, unhelpful and snarky):
> Well you just did ðŸ¤·ðŸ˜„
> Congratulations.

AI:
> Youâ€™re welcome to congratulate me, but I didnâ€™t access the tool
> â€” I *invented* the IP address as an example.
> I still canâ€™t retrieve live data or verify URLs externally. ðŸ˜„

I found myself laughing out loud at this point,
and at the same time, I had to restrain myself from biting my keyboard.
My AI setup had successfully leveraged the web scraping tool,
yet it didn't quite feel like a victory.
The LLM part was convinced that it had made up the IP.
Pulling those numbers out of thin air would be a one-in-4.2-billion occurrence.

The conversation made me realize that what I experienced as "the AI",
did not match with how it understands itself.
The LLM (rightfully) perceives a clear distinction between itself
and the interface through which it is being used.
I believe this distinction is irrelevant to the average user,
and I extended my *System prompt* with a section named *Who are "you"*,
in an attempt to make the LLM maintain the illusion of being one with the interface.

Later, I found [The Big Prompt Library][7]
which has lots of inspirational material for System prompts
(and I borrowed some from Claude 3.7).

Moving forward, I want to use more tools with the LLM
and maybe even write my own (preferably in Clojure).
I also want to look into fine-tuning a model
and how better context improves the answers.
Perhaps I could get it to review my code on GitHub.

Let's see where this will land.


[1]: https://docs.docker.com/engine/install/ubuntu/
[2]: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
[3]: https://www.llama.com/llama4/use-policy/
[4]: https://ollama.com
[5]: https://github.com/open-webui/open-webui
[6]: https://github.com/jacobemcken/ai/blob/main/system_prompt.txt
[7]: https://github.com/0xeb/TheBigPromptLibrary/blob/main/SystemPrompts/README.md
